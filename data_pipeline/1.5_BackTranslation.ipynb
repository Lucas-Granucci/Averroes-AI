{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24407232",
   "metadata": {},
   "source": [
    "# Back Translation\n",
    "\n",
    "This notebook creates parallel data by translating sentences from target languages to English using OpenAI's Batch API. It generates batch query files, submits them for processing, and formats the results into parallel corpus files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703f3cf",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dffc5",
   "metadata": {},
   "source": [
    "### Configuration and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(\"../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set up project paths\n",
    "project_root = Path.cwd().parent\n",
    "SENTENCES_DIR = project_root / config[\"SENTENCES_DIR\"]\n",
    "API_QUERIES_DIR = project_root / config[\"API_QUERIES_DIR\"]\n",
    "PARALLEL_DATA_DIR = project_root / config[\"PARALLEL_DATA_DIR\"]\n",
    "\n",
    "# Translation configuration\n",
    "API_URL = \"/v1/chat/completions\"\n",
    "MODEL = config[\"data_processing\"][\"back_translation\"][\"model\"]\n",
    "SYSTEM_PROMPT = config[\"data_processing\"][\"back_translation\"][\"system_prompt\"]\n",
    "MAX_TOKENS = config[\"data_processing\"][\"back_translation\"][\"max_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45089cf",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_translation_prompt(sentence: str, lang_name: str) -> str:\n",
    "    \"\"\"Create a translation prompt for the given sentence\"\"\"\n",
    "    return f\"Translate the following {lang_name} sentence into English:\\n{sentence}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f07ec7",
   "metadata": {},
   "source": [
    "## Step 1: Create Batch Query Files\n",
    "\n",
    "Generate JSONL files with translation queries for the OpenAI Batch API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_code, lang_config in config[\"LANGUAGES\"].items():\n",
    "    lang_sents_file = SENTENCES_DIR / f\"{lang_code}_sentences.jsonl\"\n",
    "    lang_queries_file = API_QUERIES_DIR / f\"{lang_code}_queries.jsonl\"\n",
    "\n",
    "    with open(lang_sents_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        lang_sents = [json.loads(line) for line in file]\n",
    "\n",
    "    if not lang_sents:\n",
    "        continue\n",
    "\n",
    "    with open(lang_queries_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for idx, sent in enumerate(lang_sents):\n",
    "            query_id = f\"{lang_code}_{idx}\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": create_translation_prompt(sent[\"text\"], lang_config[\"name\"])},\n",
    "            ]\n",
    "            query = {\n",
    "                \"custom_id\": query_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": API_URL,\n",
    "                \"body\": {\n",
    "                    \"model\": MODEL,\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": MAX_TOKENS,\n",
    "                },\n",
    "            }\n",
    "            out_file.write(json.dumps(query, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Batch query files created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90810f0",
   "metadata": {},
   "source": [
    "## Step 2: Submit Batch Jobs\n",
    "\n",
    "Upload query files to OpenAI and submit batch translation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_APIKEY\"))\n",
    "\n",
    "batch_info = {}\n",
    "\n",
    "for lang_code, lang_config in config[\"LANGUAGES\"].items():\n",
    "    lang_queries_file = API_QUERIES_DIR / f\"{lang_code}_queries.jsonl\"\n",
    "\n",
    "    if not lang_queries_file.exists():\n",
    "        continue\n",
    "\n",
    "    # Upload file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(lang_queries_file, \"rb\"), purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Create batch job\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": f\"backtranslation batch for {lang_code}\"},\n",
    "    )\n",
    "    batch_info[lang_code] = batch\n",
    "    print(f\"Submitted batch for {lang_code}: {batch.id}\")\n",
    "\n",
    "print(f\"\\nSubmitted {len(batch_info)} batch jobs successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5ce65",
   "metadata": {},
   "source": [
    "## Step 3: Check Batch Status\n",
    "\n",
    "Monitor the progress of submitted batch jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c136477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th: status=completed, completed=12, failed=0, total=12\n",
      "et: status=completed, completed=105, failed=0, total=105\n"
     ]
    }
   ],
   "source": [
    "for key, batch in batch_info.items():\n",
    "    batch = client.batches.retrieve(batch.id)\n",
    "    batch_info[key] = batch\n",
    "    counts = batch.request_counts\n",
    "    print(\n",
    "        f\"{key}: status={batch.status}, completed={counts.completed}, failed={counts.failed}, total={counts.total}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b471c4",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve Results and Create Parallel Data\n",
    "\n",
    "Download completed translations and create parallel corpus files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve batch responses\n",
    "batch_responses = {}\n",
    "for key, batch in batch_info.items():\n",
    "    file_response = client.files.content(batch.output_file_id)\n",
    "    batch_responses[key] = [\n",
    "        json.loads(res) for res in file_response.text.split(\"\\n\") if res\n",
    "    ]\n",
    "\n",
    "# Create parallel data files\n",
    "for lang_code, lang_config in config[\"LANGUAGES\"].items():\n",
    "    lang_sents_file = SENTENCES_DIR / f\"{lang_code}_sentences.jsonl\"\n",
    "    parallel_sents_file = PARALLEL_DATA_DIR / f\"{lang_code}-en_data.jsonl\"\n",
    "\n",
    "    with open(lang_sents_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        lang_sents = [json.loads(line) for line in file]\n",
    "\n",
    "    if not lang_sents:\n",
    "        continue\n",
    "\n",
    "    if lang_code not in batch_responses.keys():\n",
    "        continue\n",
    "\n",
    "    # Extract translations\n",
    "    translated_sentences = [\n",
    "        res[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        for res in batch_responses[lang_code]\n",
    "    ]\n",
    "    \n",
    "    assert len(translated_sentences) == len(lang_sents), (\n",
    "        f\"Mismatch: {len(translated_sentences)} translations vs {len(lang_sents)} source sentences\"\n",
    "    )\n",
    "\n",
    "    # Write parallel data\n",
    "    with open(parallel_sents_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for target, source in zip(lang_sents, translated_sentences):\n",
    "            outfile.write(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        \"target_text\": target[\"text\"],\n",
    "                        \"target_lang\": lang_code,\n",
    "                        \"source_text\": source,\n",
    "                        \"source_lang\": \"en\",\n",
    "                        \"doc_id\": target[\"doc_id\"],\n",
    "                        \"sent_id\": target[\"sent_id\"],\n",
    "                    },\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "                + \"\\n\"\n",
    "            )\n",
    "    \n",
    "    print(f\"Created parallel data for {lang_code}: {len(translated_sentences)} sentence pairs\")\n",
    "\n",
    "print(\"\\nParallel data creation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
