{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "SENTENCES_DIR = project_root / config[\"SENTENCES_DIR\"]\n",
    "EXTRACTED_DIR = project_root / config[\"EXTRACTED_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lingua import LanguageDetectorBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str, detector):\n",
    "    result = detector.detect_language_of(text)\n",
    "    return result.iso_code_639_1.name.lower() if result else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71042845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence: str):\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"#+\\s*\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\*+\", \"\", sentence)\n",
    "    sentence = re.sub(r\"\\[|\\]|\\(|\\)\", \"\", sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "\n",
    "def is_valid_sentence(sentence: str, min_length: int = 10, max_length: int = 500):\n",
    "    if len(sentence) < min_length:\n",
    "        return False\n",
    "    if len(sentence) > max_length:\n",
    "        return False\n",
    "    if not re.search(r\"[a-zA-Z\\u0080-\\uFFFF]\", sentence):\n",
    "        return False\n",
    "    if len(re.findall(r\"[a-zA-Z\\u0080-\\uFFFF]\", sentence)) < 5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_sentence_from_markdown(md_path: Path, lang_code: str, nlp):\n",
    "    text = md_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Cleaning\n",
    "    text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"\\|.*?\\|\", \"\", text)\n",
    "    text = re.sub(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf276d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_stats = []\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "\n",
    "for lang_code, lang_config in config[\"LANGUAGES\"].items():\n",
    "    lang_sents_file = SENTENCES_DIR / f\"{lang_code}_sentences.jsonl\"\n",
    "    lang_extracted_dir = EXTRACTED_DIR / lang_code\n",
    "\n",
    "    if not lang_extracted_dir.exists():\n",
    "        continue\n",
    "\n",
    "    markdown_files = list(lang_extracted_dir.glob(\"*.md\"))\n",
    "    total_sentences = 0\n",
    "\n",
    "    with open(lang_sents_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for markdown_path in tqdm(\n",
    "            markdown_files,\n",
    "            total=len(markdown_files),\n",
    "            desc=f\"Extracting {lang_config['name']}\",\n",
    "        ):\n",
    "            try:\n",
    "                md_text = markdown_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "                # Clean markdown text\n",
    "                md_text = re.sub(r\"```.*?```\", \"\", md_text, flags=re.DOTALL)\n",
    "                md_text = re.sub(r\"\\|.*?\\|\", \"\", md_text)\n",
    "                md_text = re.sub(\n",
    "                    r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "                    \"\",\n",
    "                    md_text,\n",
    "                )\n",
    "\n",
    "                doc = nlp(md_text)\n",
    "\n",
    "                sentences = []\n",
    "                for sent in doc.sents:\n",
    "                    cleaned = clean_sentence(sent.text)\n",
    "                    detected_code = detect_language(cleaned, detector)\n",
    "                    if is_valid_sentence(cleaned) and detected_code == lang_code:\n",
    "                        sentences.append(cleaned)\n",
    "\n",
    "                for idx, sentence in enumerate(sentences):\n",
    "                    data = {\n",
    "                        \"text\": sentence,\n",
    "                        \"lang\": lang_code,\n",
    "                        \"doc_id\": markdown_path.stem,\n",
    "                        \"sent_id\": idx,\n",
    "                    }\n",
    "                    out_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    total_sentences += 1\n",
    "\n",
    "                    if total_sentences >= lang_config[\"target_sentences\"]:\n",
    "                        break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error when extracting sentences: {str(e)}\")\n",
    "\n",
    "        extraction_stats.append(\n",
    "            {\n",
    "                \"Language\": lang_config[\"name\"],\n",
    "                \"Code\": lang_code,\n",
    "                \"Documents\": len(markdown_files),\n",
    "                \"Sentences\": total_sentences,\n",
    "                \"Avg per Doc\": (\n",
    "                    f\"{total_sentences / len(markdown_files):.1f}\"\n",
    "                    if markdown_files\n",
    "                    else \"0\"\n",
    "                ),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(extraction_stats))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
